---
layout: post
title: "[2019 CVPR] Gait Recognition via Disentangled Representation Learning (*incomplete*)"
date: 2019-05-27 12:00:00
tags: CV autoencoder recognition disentangled unsupervised
---

<!--more-->

---

**Table of content** (*full-version*)
[[paper]](https://arxiv.org/pdf/1904.04925.pdf)
{: class="table-of-content"}
* TOC
{:toc}

---

## Summary

- Cross reconstruction loss(서로 다른 시간의 두 영상이 들어올 때, 첫번째 영상의 appearance feature 두번째 영상의 pose feature를 이용하여 두번째 영상처럼 만드는 방법)와 gait similarity loss(같은사람이지만 다른 condition을 가진 영상들의 pose feature를 서로 유사하게 만드는 방법)를 통해서 appearance/gait feature를 disentangle하고 pose feature를 multi-layer LSTM에 통과시켜 temporal 성분이 포함된 gait feature를 추출한다.
- 고화잘의 frontal-view gait DB를 제공한다.

---

## Motivation

- 사람의 걸음걸이에 대한 feature는 외형적 변화(옷, 시점, 짐, 등)에 invariant 해야한다.
- 이런 invariant feature를 추출하기 위한 기존의 방법(GEI나 skeleton)들은 gait 정보가 부족하거나, 불필요한 정보를 생성한다. 
- 따라서 사람의 appearance/pose feature를 자동으로 disentangle하는 Gait-Net을 제안한다.

---

## Architecture

- 기존의 DR-GAN이랑 다른 점
  - Adversarial 학습 없이 novel loss function을 디자인 했다는 점
  - Pose나 appearance에 관련된 label이 필요없다는 점(label을 이산적으로 정의하기 힘듬, unsupervised)
- 전체 구조
  - Appearance/pose feature를 frame-level disentanglement 수행 후 LSTM에 의해 temporal 정보 융합
- Apperance/pose feature disentanglement
  - Cross reconstruction loss
    - 서로 다른 시간의 두 영상이 들어올 때, 첫번째 영상의 appearance feature 두번째 영상의 pose feature를 이용하여 두번째 영상처럼 만드는 방법
    - 
    $$
    \left \| \mathit{D} (f^{t_1}_a, f^{t_2}_g - I_{t_2}) \right \|^2_2
    $$
  - Gait similarity loss
    - 같은사람이지만 다른 condition을 가진 영상들의 pose feature를 서로 유사하게 만드는 방법
    - 
- Temporal aggregation
  - Incremental identity loss

---
  
## Experimental results

---

## Comments

---

## References

