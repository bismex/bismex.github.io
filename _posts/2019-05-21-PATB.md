---
layout: post
title: "[2019 CVPR] Progressive Pose Attention Transfer for Person Image Generation"
date: 2019-05-21 18:00:00
tags: re-identification generative-model attention computer-vision-application supervised gan
---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

## Summary

- 사람 영상이 주어졌을 때, 주어진 포즈로 변형하기 위한 GAN 네트워크 제안
- Pose-Attentional Transfer Blocks (PATBs)를 여러개 이어서 포즈를 점진적으로 변형
- Better appearance consistency and space consistency
- Re-identification 분야에서 데이터의 불충분 문제를 완화하기 위해 데이터셋을 확보하는 차원에서 유용하게 사용가능

## Motivation

- 어떤 사람의 모든 가능한 포즈나 뷰 영상은 하나의 manifold를 형성한다는 관점에서, 포즈가 크게 변하는 것은 manifold 상에서 이동하기 어렵지만 작은 변화는 local 수준으로 간단화할 수 있을 것이라는 가정으로 점진적인 방법을 고안
- Manifold 위에서 지역적인 이동을 하게끔 PATBs 설계

## Architecture

![picture]({{ '/assets/images/PATB000.png' | relative_url }})
{: style="width: 100%;" class="center"}

### *Generator*
- Encoder
  - 첫 번째 입력으로 condition (source) 이미지가 (N=2) down-sampling conv layer에 통과 ($$P_c \rightarrow F_0^P$$)
  - 두 번째 입력으로 condition pose (18 channel heat map) 이미지가 같은 형식의 target pose와 depth 축으로 합쳐져서 N down-sampling conv layer에 통과 ($${S_t, S_c} \rightarrow  F_0^S$$) [encoder이후에 concat하는 것보다 계산복잡도 적다는 장점]
- Pose-Attentional Transfer Network
  - 큰 틀
    - 동일한 구조의 PATB들이 cascade 형식으로 구성
    - 이미지 특징벡터와 $$F_0^P$$와 포즈 특징벡터 $$F_0^S$$가 입력되고 포즈는 버려진 채 최종 변형된 이미지 특징벡터 $$F_T^P$$만 출력
    - 하나의 PATB는 image pathway와 pose pathway로 구성되며 둘은 상호작용
  - Pose Attention Masks
    - condition pose는 어디에서 condition patch를 샘플할지, target pose는 target patch를 어디로 놓을지 가이드한다.
    - 이러한 힌트는 0~1사이의 attention mask $$M_t$$에 의해서 실현된다.
  - Image Code Update
    - 포즈에서 얻어진 $$M_t$$에 의해서 $$t-1$$번째 이미지 특징은 변형되고 또한 residual connection에 의해 일부는 보존되어 최종적으로 $$t$$번째 이미지 특징이 업데이트된다. [여러개의 PATB가 있음에도 불구하고 residual connection에 의해서 학습이 쉽게 되는 것이다.]
  - Pose Code Update
    - 이미지 코드를 다음 블락에서도 점진적으로 변화시키기 위해서 이미지 코드가 업데이트 된 것에 맞춰 포즈 코드가 업데이트 되어야 한다.
    - 포즈 코드는 $$conv_S$$에 의해서 뎁스가 작아지게 하고 이미지 코드를 depthwise concat하여 크기를 복원한다.
- Decoder
  - N deconvolution layer에 의해서 최종 출력 이미지 $$P_g$$ 생성
  
### *Discriminator*
  - 동일한 구조의 두 discriminator 이용 (appearance discriminator $$D_A$$ and shape discriminator $$D_S$$)
  - $$D_A$$는 입력 이미지 $$P_c$$와 생성된 이미지 $$P_g$$를 depth축으로 concat하여 입력을 받으며 CNN구조와 최종으로 softmax layer를 거쳐 $$R_A$$라는 appearance consistency score를 출력한다.
  - $$D_S$$는 입력 이미지와 타겟 포즈 $$S_t$$를 이용하며, 동일한 구조를 거쳐 $$R_S$$라는 shape consistency score를 출력한다.
  - 학습 과정에서 Discriminator의 capacity를 향상하기 위하여 2번의 down-sampling convolution 이후에 3번의 residual block을 거친다.
  
### *Training*
  - 최종 loss function은 adversarial loss와 combined L1 loss로 구성된다.
  - Adversarial loss는 discriminator가 real/fake를 잘 구분하게 만들기 위한 것으로 discriminator $$D_A$$와 $$D_S$$에 의한 loss이다.
    - 생성한 영상 $$P_g$$을 넣었을 때에는 fake(0)으로 판단하도록 / 타겟 영상 $$P_t$$를 넣었을 때에는 real(1)로 판단하도록
  - Combined L1 loss는 generator가 생성한 영상 $$P_g$$이 타겟 영상 $$P_t$$과 최대한 비슷하게 만들기 위한 것으로 일반적인 pixelwise L1 loss와 Perceptual L1 loss를 사용한다. [포즈 distortion을 감소하고 이미지를 보다 자연스럽게 만들기 위해]
    - Pretrained VGG-19 모델 conv1_2에서 출력된 특징을 비교
  
  
  



## Experimental results

## Comments


## References

[1] Zhu, Zhen, et al. "Progressive Pose Attention Transfer for Person Image Generation." arXiv preprint arXiv:1904.03349 (2019).

