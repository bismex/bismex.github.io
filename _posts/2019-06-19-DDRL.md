---
layout: post
title: "[2019 CVPR] Learning to Reduce Dual-level Discrepancy for Infrared-Visible Person Re-identification"
date: 2019-06-19 20:00:00
tags: CV 
---

<!--more-->

---

**Table of content** (*full-version*)
[[paper]](http://homepage.ntu.edu.tw/~r06944046/pdf/Wang_Learning_to_Reduce_Dual-level_Discrepancy_for_Infrared-Visible_Person_Re-Identification_CVPR19.pdf)
{: class="table-of-content"}
* TOC
{:toc}

---

## Summary

- Infrared-Visible person RE-ID (IV-REID) 분야
- 문제점: appearance(APP) discrepancy + modality(MOD) discrepancy
- 기존방법: feature-level contraint 사용하여 APP+MOD 차이를 동시에 감소 시도
- 논문방법: Dual-level Discrepancy Reduction Learning (DDRL) 는 두 개의 discrepancy를 분리
  - Image-level sub-network: RGB $$\leftrightharpoons$$ IR 영상 변환으로 서로 다른 MOD를 가진 영상에 대한 표현을 통합 
  - Feature-level sub-network: 통합된 multi-spectral 영상을 가지고 feature embedding을 통해 남은 APP discrepancy를 감소
  - 두 개의 sub-network를 직렬로 jointly 학습하여 dual-level discrepancy를 협동적 + 조심스럽게 감소

---

## Motivation

- IR-REID 에서 Intra-person 사이의 간격이 Inter-person 사이의 간격보다 오히려 커서 성능이 낮음
- 이를 feature-space에서 한 번에 조정하기 힘듦
- 영상 변환을 통해 unified space로 옮긴 다음에 feature-space에서 남은 appearance의 차이를 더 줄임

<br/>
![picture]({{ '/assets/images/DDRL01.png' | relative_url }})
{: style="width: 100%;" class="center"}
<span style="color: #e01f1f;">***Fig. 1. Motivation***</span>

---

## Architecture

- 전체 프레임워크
  - Image-level discrepancy reduction
    - Style disentanglement($$L_{{VAE}_v}, L_{{VAE}_i}$$): VAE 이용(KL loss + self recons loss), RGB / IR 영상 따로 Enc, Dec 존재
    - Domain specific image generation($$L_{{GAN}_v}, L_{{GAN}_i}$$): GAN 이용(adversarial loss), 
    - Cycle-consistency($$L_{{CC}_v}, L_{{CC}_i}$$): cross reconstruction loss($$x \leftrightharpoons \hat(y), y \leftrightharpoons \hat(x)$$
    - Modality unification: RGB, IR에서 영상을 합치지 않고 multi-spectral image (4dim$$\times$$2)를 생성, 각 batch마다 sample set(여러 장의 multi-spectral image)을 발생
    - Objective for training: 6개의 loss를 학습하기 위해서 그리고 multi-spectral 영상을 생성하기 위해서 RGB $$x$$와 IR버전 $$\hat(x)$$, IR $$y$$와 RGB버전 $$\hat(y)$$가 필요하다. Query-gallery 영상을 포함한 모든 이미지를 이런 방식으로 나타내어 학습하고 이 때 modality discrepancy가 크게 감소된다. 
  - Feature-level discrepancy reduction
    - ResNet-50사용, last FC-1000 $$\rightarrow$$ FC-1024로 대체
    - FC-1024 이후에 두 개의 독립적인 FC-128(for triplet loss), FC-$$N_p$$(for cross-entropy loss)로 분할

<br/>
![picture]({{ '/assets/images/DDRL02.png' | relative_url }})
{: style="width: 100%;" class="center"}
<span style="color: #e01f1f;">***Fig. 2. 전체 프레임워크***</span>




---
  
## Experimental results


<br/>
![picture]({{ '/assets/images/DDRL03.png' | relative_url }})
{: style="width: 100%;" class="center"}
<span style="color: #e01f1f;">***Fig. 3. IV-REID 성능 비교***</span>

<br/>
![picture]({{ '/assets/images/DDRL04.png' | relative_url }})
{: style="width: 60%;" class="center"}
<span style="color: #e01f1f;">***Fig. 4. Ablation study***</span>

<br/>
![picture]({{ '/assets/images/DDRL05.png' | relative_url }})
{: style="width: 100%;" class="center"}
<span style="color: #e01f1f;">***Fig. 5. Intra-person, inter-person의 feature-space distribution***</span>


<br/>
![picture]({{ '/assets/images/DDRL06.png' | relative_url }})
{: style="width: 60%;" class="center"}
<span style="color: #e01f1f;">***Fig. 6. Image-space 변환결과***</span>

<br/>
![picture]({{ '/assets/images/DDRL07.png' | relative_url }})
{: style="width: 60%;" class="center"}
<span style="color: #e01f1f;">***Fig. 7. Failure case***</span>


---

## Comments

---

## References

[1] Wang, Zhixiang, et al. "Learning to Reduce Dual-level Discrepancy for Infrared-Visible Person Re-identification." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Vol. 2. No. 3. 2019.
