---
layout: post
title: "ALL about Deep Generative Model (VAE, GAN) (ongoing..)"
date: 1990-07-19 13:15:00
tags: ML GAN AE  
---

<!--more--> 

---
 
**Table of content**
{: class="table-of-content"}
* TOC
{:toc}

---

## AE (Autoencoder) [2006]
**[[paper](https://pdfs.semanticscholar.org/c50d/ca78e97e335d362d6b991ae0e1448914e9a3.pdf)][[blog-KR](https://booiljung.github.io/machine_learning_notes/auto_encoders.html)][[blog-EN](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)]**

### 1) DAE (Denoising-AE) [2008]
**[[paper1](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)][[paper2](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)][[blog-EN](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)]**

### 2) SAE (Sparse-AE) [2011]
**[[paper1](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)][[paper2](https://arxiv.org/pdf/1312.5663.pdf)][[blog-EN](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)]**

### 3) CAE (Contractive-AE) [2011]
**[[paper](http://www.icml-2011.org/papers/455_icmlpaper.pdf)][[blog-EN](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)]**
- 레이블 정보를 알 때 VAE를 학습하는 방법이다. 
- ELBO 식은 conditional probability만 조금 다르고 동일하다.
- 기존의 VAE는 x에서 직접적으로 z를 출력하고 z에서 직접적으로 x를 생성하기에 특정 데이터 생성 불가능하다.
- 레이블을 유지한 채 z(스타일)을 바꾸면 MNIST인 경우 회전되거나 두께가 변한다.

### 4) AAE (Advarsarial-AE) [2015]
**[[paper](https://arxiv.org/pdf/1511.05644.pdf)][[blog-EN1](https://www.inference.vc/adversarial-autoencoders/)][[blog-EN2](https://hjweide.github.io/adversarial-autoencoders)][[blog-EN3](https://towardsdatascience.com/paper-summary-adversarial-autoencoders-f89bfa221e48)]**
- 기존 VAE에서는 KL계산상의 어려움으로 encoder가 normal distribution으로 가정했었다.
- VAE의 KL기능(KL term은 q가 p에 가깝게 확률분포를 변화시키는 과정) 대신에 GAN loss에 의해서 manifold를 N(0,1)이 아닌 다른 임의의 함수(모델 선택의 폭이 넓어진다)로 표현이 가능하다.
- VAE에 비해서 image-level 분포가 아닌 latent-space 분포를 같게 한다는 아이디어이다.
- Discriminator는 p(z)로부터의 샘플을 pos로, q(z)를 neg로 구분한다.
- Manifold 모양을 볼 때 VAE는 자주 나오는 값을 중시하여 빈 공간이 가끔 있지만, AAE는 분포의 모양을 중시하여 빈공간이 적다.
- Loss
  - 학습1(update AE $$\rightarrow$$ $$\phi, \theta$$): 
  $$
  L_i(\phi, \theta, x_i) = - \mathbb{E}_{q_\phi (z|x_i)} [log(p_\theta (x_i|z))]
  $$
  - 학습2(update D $$\rightarrow$$ $$\lambda$$): 
  $$
  -V_i(\phi, \lambda, x_i, z_i) = - log(d_\lambda (z_i))-log(1-d_\lambda (q_\phi (x_i)))
  $$
  - 학습3(update G $$\rightarrow$$ $$\phi$$): 
  $$
  -V_i(\phi, \lambda, x_i, z_i) = - log(d_\lambda (q_\phi (x_i)))
  $$

### 5) DC-IGN (Deep Convolutional Inverse Graphics Network) [2015]
**[[paper](http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf)][[blog-KR](https://lyusungwon.github.io/generative-models/2018/05/01/dcign.html)]**
- 영상의 표현을 해석가능하게 만들어 원하는 영상의 생성이 가능하다.
- Disentanglement 논문의 초기 버전이다.
- 원하는 variation만을 갖는 데이터셋을 얻기가 힘들다는 한계점이 있다. 









---------------------





## VAE (Variational-AE) [2014]
**[[paper](https://arxiv.org/pdf/1312.6114.pdf)][[blog-KR1](https://blog.naver.com/atelierjpro/220981354861)][[blog-KR2](https://blog.naver.com/ynca333/221315157737)][[blog-KR3](https://blog.naver.com/stykworld/221374794894)][[blog-EN1](https://www.quora.com/What-are-the-pros-and-cons-of-Generative-Adversarial-Networks-vs-Variational-Autoencoders)][[blog-EN2](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)][[video-KR](https://www.youtube.com/watch?v=KYA-GEhObIs&t=1522s)][[ppt-KR](https://mega.nz/#!tBo3zAKR!yE6tZ0g-GyUyizDf7uglDk2_ahP-zj5trVZSLW3GAjw)][[video-EN1](https://www.youtube.com/watch?v=9zKuYvjFFS8)][[video-EN2](https://www.youtube.com/watch?v=uaaqyVS9-rM)]**


### 1) C-VAE (Conditional-VAE) [2015]
**[[paper](http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)][[blog-KR1](https://blog.naver.com/stykworld/221377790514)][[blog-KR2](https://ratsgo.github.io/generative%20model/2018/01/28/VAEs/)][[blog-EN](https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/)][[video-EN](https://www.youtube.com/watch?v=7Tlk3Gql-Wg)]**


### 2) ELBO [2016]
**[[paper1](http://approximateinference.org/2016/accepted/HoffmanJohnson2016.pdf)][[paper2](https://arxiv.org/pdf/1711.00464.pdf)]**


### 3) L-VAE (Ladder VAE) [2016]
**[[paper](https://arxiv.org/pdf/1602.02282.pdf)][[blog-KR](https://m.blog.naver.com/PostView.nhn?blogId=hist0134&logNo=221048568154&proxyReferer=https%3A%2F%2Fwww.google.com%2F)]**

- VAE를 어떻게 학습시킬 것인가.


### 4) Beta-VAE [2017]
**[[paper1](https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf)][[paper2](https://arxiv.org/pdf/1804.03599.pdf)][[blog-KR](https://lyusungwon.github.io/generative-models/2018/05/03/ubvae.html)][[blog-EN1](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)][[blog-EN2](https://medium.com/uci-nlp/summary-beta-vae-learning-basic-visual-concepts-with-a-constrained-variational-framework-91ad843b49e8)]**

### 5) Info-VAE [2017] 
**[[paper](https://arxiv.org/pdf/1706.02262.pdf)][[blog-KR](https://shryu8902.github.io/_posts/2018-12-03-paper-infovae/)][[blog-EN1](https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd)][[blog-EN2](https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/)]**

### 6) Semi-VAE [2017]
**[[paper](http://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf)][[blog-EN](http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/)]**

### 7) K-VAE (Kalman VAE) [2017]
**[[paper](http://papers.nips.cc/paper/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning.pdf)]**

### 8) Factor-VAE [2018]
**[[paper](https://arxiv.org/pdf/1802.05983.pdf)]**


### 9) Beta-TCVAE (Beta Total Correlation VAE) [2018]
**[[paper](http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders.pdf)]**


### 10) DIP-VAE (Disentangled Inferred Prior VAE) [2018]
**[[paper](https://arxiv.org/pdf/1711.00848.pdf)][[blog-EN](https://www.ibm.com/blogs/research/2018/05/disentanglement-deep-learning/)]**


### 11) ML-VAE (Multi-level VAE) [2018]
**[[paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16521/15918)]**


### 12) S-VAE (Hyperspherical-VAE) [2018]
**[[paper](https://arxiv.org/pdf/1804.00891.pdf)]**

### 13) DDPAE (Decompositional Disentangled Predictive AE) [2018]
**[[paper](http://papers.nips.cc/paper/7333-learning-to-decompose-and-disentangle-representations-for-video-prediction.pdf)][[poster](https://jthsieh.github.io/files/publications/NIPS18_Poster.pdf)]**









---------------------







## GAN (Generative Adversarial Network) [2014]
**[[paper](https://arxiv.org/pdf/1406.2661.pdf)][[[GAN-family](https://github.com/hindupuravinash/the-gan-zoo)]]**

### 1) CGAN (Conditional GAN) [2014]
**[[paper](https://arxiv.org/pdf/1411.1784.pdf)]**

### 2) DCGAN (Deep Convolutional GAN) [2015]
**[[paper](https://arxiv.org/pdf/1511.06434.pdf)]**

### 3) CatGAN (Categorical GAN) [2015]
**[[paper](https://arxiv.org/pdf/1511.06390v2.pdf)]**

### 4) Context Encoder [2016]
**[[paper](https://arxiv.org/pdf/1604.07379.pdf)]**

### 5) CoGAN (Coupled GAN) [2016]
**[[paper](https://arxiv.org/pdf/1606.07536v2.pdf)]**

### 6) ImprovedGAN [2016]
**[[paper](https://arxiv.org/pdf/1606.03498.pdf)]**

### 7) InfoGAN (Information Maximizing GAN) [2016]
**[[paper](https://arxiv.org/pdf/1606.03657.pdf)]**

### 8) EBGAN (Energy-based GAN) [2016]
**[[paper](https://arxiv.org/pdf/1609.03126v4.pdf)]**

### 9) ACGAN (Auxiliary Classifier GAN) [2016]
**[[paper](https://arxiv.org/pdf/1610.09585.pdf)]**

### 10) pix2pix [2016]
**[[paper](https://arxiv.org/pdf/1611.07004.pdf)]**

### 11) LSGAN (Least Squares GAN) [2016]
**[[paper](https://arxiv.org/pdf/1611.04076v3.pdf)]**

### 12) SimGAN (Simulated GAN) [2016]
**[[paper](https://arxiv.org/pdf/1612.07828.pdf)]**

### 13) StackGAN [2016]
**[[paper](https://arxiv.org/pdf/1612.03242.pdf)]**

### 14) WGAN (Wasserstein GAN) [2017]
**[[paper](https://arxiv.org/pdf/1701.07875.pdf)]**

### 15) WGAN-GP (WGAN - Gradient Penalty) [2017]
**[[paper](https://arxiv.org/pdf/1704.00028.pdf)]**

### 16) SEGAN (Speech Enhancement GAN) [2017]
**[[paper](https://arxiv.org/pdf/1703.09452v3.pdf)]**

### 17) CycleGAN (Cycle consistent with GAN) [2017]
**[[paper](https://arxiv.org/pdf/1703.10593.pdf)]**

### 18) DiscoGAN (Discover Cross-Domain Relations with GAN) [2017]
**[[paper](https://arxiv.org/pdf/1703.05192v2.pdf)]**

### 19) LRGAN (Layered Recursive GAN) [2017]
**[[paper](https://arxiv.org/pdf/1703.01560v3.pdf)]**

### 20) TripleGAN [2017]
**[[paper](https://arxiv.org/pdf/1703.02291v4.pdf)]**

### 21) ProGAN (Progressive Growing of GAN) [2017]
**[[paper](https://arxiv.org/pdf/1710.10196.pdf)]**

### 22) BicycleGAN [2017]
**[[paper](https://arxiv.org/pdf/1711.11586.pdf)]**

### 23) StarGAN [2017]
**[[paper](https://arxiv.org/pdf/1711.09020.pdf)]**

### 24) SNGAN (Spectral Normalization GAN) [2018]
**[[paper](https://arxiv.org/pdf/1802.05957.pdf)]**

### 25) SAGAN (Self-Attention GAN) [2018]
**[[paper](https://arxiv.org/pdf/1805.08318.pdf)]**

### 26) BIGGAN (Large Scale GAN) [2018]
**[[paper](https://arxiv.org/pdf/1809.11096.pdf)]**

### 27) StyleGAN (Style-based Generator Architecture for GAN) [2018]
**[[paper](https://arxiv.org/pdf/1812.04948.pdf)]**


