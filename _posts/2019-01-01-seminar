---
layout: post
title: "Short review (Paper seminar)"
date: 2019-01-01 00:00:00
tags: CV ML
---

<!--more-->

---

**Table of content** (*short-version*)
{: class="table-of-content"}
* TOC
{:toc}

---

★★★의 개수는 내가 적용할 수 있는가에 관련된 것. 논문 및 발표의 퀄리티가 아님 

Learning Across Tasks and Domains [good]

Multitask 관련 논문
네트워크 설명
task 두개 domain 두개
A synthetic (task12 label있음)
B real (task1에 대한 label만 있음)
빨간색을 task 1 (segmentation) AB가지고 학습
초록색은 task 2 (depth estimation) A만 가지고 학습, knowledge transfer (G) 
총 4 step 학습
기존 DA는 data 분포 비슷하게 하는 것과 달리 domain아닌 task에 대한 feature 비슷하게 맞춰준다.
Loss는 task 1 L1 loss(segmentation), G는 align 잘해주기 위해서 (task 1 enc feature, task2 enc feature L2 loss align)
더 깊은 layer에서 task feature align 해주는게 좋다.
G구조는 정확히 안나와있고 conv relu bn 사용, enc-dec구조 (feature 반 줄이고)
[두개 task가 있을 때, 하나의 label이 없을 경우 사용 가능]
[분할 학습하는 방법에 대한 것]
[★★★★좀더 고민이 필요할듯]

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [good]

Meta learning관련 (learning to learn)
분야설명-
few shot때 일반적인 SGD는 오버피팅 된다.
어떻게 학습을 해야 adaptation이 잘 될까?
small training set으로 모델 학습할 때 잘 adaptation 될 수 있도록
classification 관련 metalearning 논문
접근방식: initial parameter를 잘 만들겠다. 
배치마다 loss 계산한다음에 파라미터 업데이트 하는게 기본이지만,
업데이트하지않고 변경된 파라미터에 대한것로 다시 loss를 구한다음 이게 작아지는 방향으로 업데이트
regression, classification, reinforce learning 적용
미래를 봐서 업데이트를 했다고 가정하고 그 때의 최적에 방향으로 갈 수 있게끔 업데이트함.
[★★★★★pretrain할 때 이방법을 통해 하는게 나을수도 있다.]
[★★★★★나중에 학습할때도 이걸로 사용하곤 한다.]

Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles [good]

아래의 self supervised를 적용하는 것.
이미지를 9개의 타일로 자른 다음에, 맞추는 것. (직소퍼즐, 조합을 label로 생각해서 직접 만듬)
(이는 sementic context를 파악하는 데 좋다.)
이 파라미터를 가지고 transfer learning을 해보고, supervised learning을 해 봤을 때, 성능이 올라감.
특히 여기는 unsupervised classification을 위해서 제안한 것.
human labeling을 쓰지 않는 것이 핵심
[★★★unsup 문제를 풀거면 이런 쪽으로 다른 task를 생각해보라!]


Self-supervised learning 간략 설명
- unsup 한계를 극복하기 위해서
- ground truth는 strong supervision. 얻기 힘들다.
- Unsup이랑 다른점은 unsup안에 selfsup이 포함되어 있고 label, domain정보 전혀 없이 학습하는 것.
- 도메인에 대한 정보를 가지고 임의의 task(pre-task)를 먼저 학습하고, 추가로 sup 학습하는 것


NLNL: Negative Learning for Noisy Labels
- 데이터셋에 잘못된 label이 존재할 때 주어진 레이블로 학습하는 것이 아닌, rand sampling을 통해 하나의 다른 label을 부여하고 그 label이 아니다 (negative learning) 라는 방식의 CE loss를 사용
- NL만 하면 수렴속도가 너무 느려서 NL + PL (원래 CE) 섞음
- 또한 confidence (CE의 max predicted prob.) 를 고려해서 NL을 하는 selective learning을 수행
- 학습은 NL -> SelNL -> SelPL 하는 방식으로 진행하면 좋은 성능
- cipar100이나 10으로 label이 적을 때 잘되는 것 같음(imagenet에서 한계성)
- 최종 방법은 semisupervised까지 적용함
[★★★★Negative mining같은 느낌이기도 한데 다른 task에서도 적용가능할듯.]


ABD-Net: Attentive but Diverse Person Re-Identification
- Attention 기반 reid 논문 
- 기존의 attention기반 방법들은 영상을 local하게 보기 때문에 feature correlation이 너무 높아짐
- 이는 feature representation이 중요한 reid task에서 단점이 될 수 있다.
- lower correlation을 갖도록 CVDO를 제안 (regularization?)
- attention은 channel간의 유사도를 보는 CAM과 position간의 유사도를 보는 PAM 적용
- PAM은 non-local과 거의 동일, CAM도 기존에 있는것 (그 CAM 아님)
[★attention에 크게 관심은 없어서..]


Partially-Supervised Image Captioning
- 학습할 때 보이지 않았던 것을 생성하는 방법
- FSM(Finite state machine) 을 이용
[★captioning에 크게 관심은 없어서..]


Tracking emerges by colorizing videos
- Self supervised learning
- 흑백영상을 RGB로 바꾸는 과정을 통해 여러가지 태스크를 수행
- RGB영상이 없는 흑백 영상 / RGB영상이 있는 흑백 영상(n-1, n-2, n-3) 이용
- 흑백 영상간의 유사 point를 찾은다음 해당하는 color를 가져온다.
- 학습은 이렇게 수행하지만 실제 테스트때는 color가 아닌 segmentation mask나 pose keypoint 추정을 수행
- One-shot learning


EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- 가장 좋은 성능
- 파라미터도 적음
- Base(B0)는 Nas방법을 통해 찾음
- B0를 scaling해서 더 좋은 것을 찾는다.
- width scaling / depth scaling / high-resulution scaling등을 합침


## References

[1] 



