---
layout: post
title: "[2019 CVPR] Adaptive Transfer Network for Cross-Domain Person Re-Identification"
date: 2019-07-25 12:00:00
tags: CV REID unsupervised DA ensemble GAN
---

<!--more-->

---

**Table of content** (*full-version*)
[[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Adaptive_Transfer_Network_for_Cross-Domain_Person_Re-Identification_CVPR_2019_paper.pdf)
{: class="table-of-content"}
* TOC
{:toc}

---

## Summary

- Unsupervised reid 논문
- Domain gap을 줄이기 위해서 여러개의 본질적인 factor로 구분한 첫 논문
- Source와 target의 domain gap을 줄이기 위한 전략으로 divide and conquer 방식 제안
  - Factor-wise sub-transfers: 특정 factor(illumination, resolution, camera-view)에 집중하여 style을 변형
  - Adaptive ensemble method: 여러개의 factor-wise transfer에 가중치를 부여하며 융합

---

## Architecture

- Problem formulation
  - Source: pairwise label 있음, 이를 target처럼 변경한 다음 supervised 방식으로 학습
  - Target: pairwise label 없음, 영상 스타일을 제공
  - Objective: 소스 영상을 타켓처럼 변형한 다음, Jensen-Shannon divergence를 통해 데이터 분포를 유사하게 만든다.
- 전체 아키텍처
  - 세 개의 모듈로 구성 (factorGAN 3개, ensemble GAN, selection network)
  - Transfer network (factorGAN, ensemble GAN)
    - 공통구조
      - CycleGAN (2개의 generator-generator pairs) 기반이지만 (adversarial loss, cycle-consistent loss, identity mapping loss)를 사용하지는 않음
      - 실제로 [2]와 비슷
      - Loss: $$L_{gan} = L_{adv} + \lambda_1 \cdot L_{cyc} + \lambda_2 \cdot L_{ide}$$
        - Adv: 변환된 source 데이터 분포를 target 데이터 분포에 매칭
        - Cyc: 원본 샘플이 두 번의 cycle 변환 이후에도 복원되도록
        - Ide: 원본과 변환된 샘플 사이의 color consistency
    - FactorGAN
      - Source data의 영상 가공을 통해 한 쌍의 영상을 만든 다음 이를 미리 학습시킨다.
      - Illumination: random gamma correation / Resolution: downsample / Camera-view: 2 images from different camera in S
      - Illumination GAN: 기본 loss + illumination constraint (using illumination insensitive feature) [3]
      - Resolution GAN: 기본 loss + resolution constraint (using resolution-insensitive feature) [4]
      - Camera-view GAN: 기본 loss만 이용
    - Ensemble GAN
      - Factor GAN 의 latent vector와 selection network에서 추출된 가중치 이용하여 concatenate
      - 1by1 convolution으로 인해 다시 dimension을 낮춘 후 decoder를 통해 최종 영상 복원
      - 하나의 discriminator를 이용하여 real/fake 판단
      - Jensen-Shannon divergence를 이용하여 factorGAN에 의해서 추출된 latent vector 쌍(총 3개, 경우의 수 3C2) 사이의 데이터 분포 맞춤
      - 최종 loss는 기본 loss + JS loss
    - 모듈구조
      - Decoder: 9개의 residual block, 4개의 conv laer [5]
      - Discriminator: 70 $$\times$$ 70 PatchGANs [6]
      - Detailes can be found in [7]
      - Decoder, discriminator는 웨이트 공유, encoder만 다른 웨이트
  - Selection network
    - FactorGAN 사이의 가중치 계산하는 네트워크
    - Training
      - 입력(학습): 한 쌍의 source + target 영상
      - 구조: 4개의 convolution + 1개의 FC (6-dim) $$\rightarrow$$ 2개의 softmax
      - 출력: 두 쌍의 이미지에 대한 3+3개의 가중치
      - loss: MSE (어떤 가중치가 적용되었을 때 변환 결과가 목표와 유사한지)
    - Testing
      - 한개의 source가 입력되면 3개의 가중치가 계산되어 3개의 factorGAN과 연결
  - Feature learning
    - 위의 네트워크에 의해서 target-like source 영상들이 생성
    - 이를 통해 supervised 방식과 같이 학습
    - ResNet50(2048-dim), GoogleNet(4096-dim) 이용

<br/>
<p align="center" style="color: #e01f1f; font-weight: bold;">[전체 아키텍처]</p>
![picture]({{ '/assets/images/ATNET01.png' | relative_url }}){: style="width: 100%;" class="center"}
<br/>





---
  
## Experimental results



<br/>
<p align="center" style="color: #e01f1f; font-weight: bold;">[영상 변환 결과 예시]</p>
![picture]({{ '/assets/images/ATNET02.png' | relative_url }}){: style="width: 70%;" class="center"}
<br/>


<br/>
<p align="center" style="color: #e01f1f; font-weight: bold;">[영상 융합 결과 예시]</p>
![picture]({{ '/assets/images/ATNET03.png' | relative_url }}){: style="width: 70%;" class="center"}
<br/>

---

## Comments

---

## References

[1] Liu, Jiawei, et al. "Adaptive Transfer Network for Cross-Domain Person Re-Identification." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.

[2] [30]

[3] [39]

[4] [13]

[5] [8]

[6] [10] 

[7] [48]
